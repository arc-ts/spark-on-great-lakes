{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afbc7396",
   "metadata": {},
   "source": [
    "# Data Science Examples\n",
    "This repo contains example code that demonstrates the data science software available on Great Lakes and Cavium ThunderX. The code utilizes Spark, Python and other libraries commonly used for data science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b064d85",
   "metadata": {},
   "source": [
    "## Select Cluster\n",
    "Run the cell below to create a dropdown menu, then select which cluster your are running on, Great Lakes or ThunderX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423127f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "# Prompt user to select cluster for Great Lakes or Cavium ThunderX\n",
    "cluster = widgets.Dropdown(options = ['greatlakes', 'thunderx'], description = \"Cluster\")\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69906c27",
   "metadata": {},
   "source": [
    "## Approximate Pi\n",
    "Performs parallel computation of Pi using some example code included with Spark.\n",
    "\n",
    "Spark creates 1,500 partitions, creates a task per partition, and divides the tasks between the available executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879a8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "partitions = 1500\n",
    "n = 100000 * partitions\n",
    "\n",
    "def f(_):\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "start = time.time()\n",
    "count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Pi was computed using Spark in {} seconds. Pi is roughly {}.\"\n",
    "    .format(round(end - start, 2), round(4.0 * count / n, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f27468",
   "metadata": {},
   "source": [
    "## Word Count\n",
    "Performs a word count on a text file with Spark RDD.\n",
    "\n",
    "Spark reads the dataset (1.2 GB) and automatically splits it into 9 partitions based on file size. It assigns the tasks to the available executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132bafd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create an RDD in PySpark from large text file\n",
    "start = time.time()\n",
    "if cluster.value == \"greatlakes\":\n",
    "    input_text_file = \"/nfs/turbo/arcts-data-hadoop-stage/data/Gutenberg.txt\"\n",
    "elif cluster.value == \"thunderx\":\n",
    "    input_text_file = \"/data/Gutenberg.txt\"\n",
    "else:\n",
    "    print(\"Error: The var 'cluster' should be set to 'greatlakes' or 'thunderx'.\")\n",
    "rdd = sc.textFile(input_text_file)\n",
    "\n",
    "# Create function to make it all lower-case and split the lines into words,\n",
    "# creating a new RDD with each element being a word.\n",
    "def Func(lines):\n",
    "    lines = lines.lower()\n",
    "    lines = lines.split()\n",
    "    return lines\n",
    "\n",
    "rdd_flat  = rdd.flatMap(Func)\n",
    "\n",
    "# Do a word count using a map-reduce like function.\n",
    "# Map each word with a count of 1 like a key-value pair where the value is 1.\n",
    "rdd_mapped = rdd_flat.map(lambda x: (x,1))\n",
    "# Then group each count by key.\n",
    "rdd_grouped = rdd_mapped.groupByKey()\n",
    "# Take the sum of each word, then swap the key value pair order,\n",
    "# then sort by value instead of key.\n",
    "rdd_frequency = rdd_grouped.mapValues(sum).map(lambda x: (x[1],x[0])).sortByKey(False)\n",
    "# Get the 10 most frequent words.\n",
    "top_ten = rdd_frequency.take(10)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Top 10 most frequent words were found with Spark in {} seconds.\".format(round(end - start, 2)))\n",
    "print(\"Most frequent words: {}\".format(top_ten))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618438d5",
   "metadata": {},
   "source": [
    "## Test NumPy\n",
    "Performs NumPy's mod() function on large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33028a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def mod(x):\n",
    "    import numpy as np\n",
    "    return (x, np.mod(x, 2))\n",
    "\n",
    "# Create an RDD of series of integers with 20 partitions\n",
    "start = time.time()\n",
    "rdd = sc.parallelize(range(10000000), 30)\n",
    "print(\"Created RDD of {0} partitions.\".format(rdd.getNumPartitions()))\n",
    "\n",
    "# Apply the mod function to each integer and get max\n",
    "rdd.map(mod).max()\n",
    "end = time.time()\n",
    "print(\"Test completed in {0} seconds.\".format(round(end - start, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ee5fba",
   "metadata": {},
   "source": [
    "## Numeric Integration\n",
    "Performs a numeric integration with SciPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Python Program with NumPy and SciPy\n",
    "# Basic Numerical Integration: the Trapezoid Rule\n",
    "# https://nbviewer.jupyter.org/github/ipython/ipython/blob/master/examples/IPython%20Kernel/Trapezoid%20Rule.ipynb\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "from scipy.version import version\n",
    "\n",
    "# Use NumPy to define a simple function and sample it between 0 and 10 at 200 points\n",
    "def f(x):\n",
    "    return (x-3)*(x-5)*(x-7)+85\n",
    "\n",
    "start = time.time()\n",
    "x = np.linspace(0, 10, 200)\n",
    "y = f(x)\n",
    "\n",
    "# Use NumPy to choose a region to integrate over and take only a few points in that region\n",
    "a, b = 1, 8 # the left and right boundaries\n",
    "N = 5 # the number of points\n",
    "xint = np.linspace(a, b, N)\n",
    "yint = f(xint)\n",
    "\n",
    "# Compute the integral both at high accuracy and with the trapezoid approximation\n",
    "\n",
    "# Use SciPy to calculate the integral\n",
    "integral, error = quad(f, a, b)\n",
    "print(\"The integral is:\", integral, \"+/-\", error)\n",
    "\n",
    "# Use NumPy to calculate the area with the trapezoid approximation\n",
    "integral_trapezoid = sum( (xint[1:] - xint[:-1]) * (yint[1:] + yint[:-1])\n",
    "    ) / 2\n",
    "print(\"The trapezoid approximation with\", len(xint),\n",
    "    \"points is:\", integral_trapezoid)\n",
    "end = time.time()\n",
    "print(\"Test completed in {0} seconds.\".format(round(end - start, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8c92b3",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "Performs a logistic regression using Spark MLlib with example code included with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd074ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])\n",
    "\n",
    "start = time.time()\n",
    "if cluster.value == \"greatlakes\":\n",
    "    input_svm_text_file = \"/nfs/turbo/arcts-data-hadoop-stage/data/sample_svm_data.txt\" # Great Lakes\n",
    "elif cluster.value == \"thunderx\":\n",
    "    input_svm_text_file = \"/data/sample_svm_data.txt\"                                   # ThunderX\n",
    "else:\n",
    "    print(\"Error: The var 'cluster' should be set to 'greatlakes' or 'thunderx'.\")\n",
    "data = sc.textFile(input_svm_text_file)\n",
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "# Build the model\n",
    "model = LogisticRegressionWithLBFGS.train(parsedData)\n",
    "\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(parsedData.count())\n",
    "end = time.time()\n",
    "\n",
    "print(\"Logistic regression was completed in {0} seconds.\".format(round(end - start, 2)))\n",
    "print(\"Training Error = \" + str(trainErr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fc5dfa",
   "metadata": {},
   "source": [
    "## Natural Language Toolkit\n",
    "Performs word tokenizing and part of speech tagging with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f5413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "partitions = 12\n",
    "nltk_data_location = \"/nfs/turbo/arcts-data-hadoop-stage/data/nltk_data/\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "if cluster.value == \"greatlakes\":\n",
    "    input_text_file = \"/nfs/turbo/arcts-data-hadoop-stage/data/complete-works-of-shakespeare.txt\"    \n",
    "elif cluster.value == \"thunderx\":\n",
    "    input_text_file = \"/data/complete-works-of-shakespeare.txt\"    \n",
    "else:\n",
    "    print(\"Error: The var 'cluster' should be set to 'greatlakes' or 'thunderx'.\")\n",
    "\n",
    "data = spark.sparkContext.textFile(input_text_file, partitions)\n",
    "\n",
    "def word_tokenize(x):\n",
    "    import nltk\n",
    "    nltk.data.path.append(nltk_data_location)\n",
    "    return nltk.word_tokenize(x)\n",
    "\n",
    "def pos_tag(x):\n",
    "    import nltk\n",
    "    nltk.data.path.append(nltk_data_location)\n",
    "    return nltk.pos_tag([x])\n",
    "\n",
    "# Tokenize\n",
    "words = data.flatMap(word_tokenize)\n",
    "\n",
    "# Label parts of speech\n",
    "pos_word = words.map(pos_tag)\n",
    "print(pos_word.take(20))\n",
    "end = time.time()\n",
    "\n",
    "print(\"Completed dataset tokenization and part of speech tagging of first 20 elements in {0} seconds.\"\n",
    "      .format(round(end - start, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b8620b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
